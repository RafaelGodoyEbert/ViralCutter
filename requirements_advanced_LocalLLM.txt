-r requirements.txt

# Local LLM Support with CUDA 12.4
--extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124
llama-cpp-python
